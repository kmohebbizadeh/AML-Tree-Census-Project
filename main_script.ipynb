{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e457747",
   "metadata": {},
   "source": [
    "# Tree Census Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f4338",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, make_scorer\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9b56d",
   "metadata": {},
   "source": [
    "## Exlporatory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data with relative path\n",
    "tree_df = pd.read_csv('2015_Street_Tree_Census_-_Tree_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84358b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot missing values by columns\n",
    "vals = (tree_df.isna().sum() * 100 / len(tree_df)).values\n",
    "keys = tree_df.columns\n",
    "plt.tight_layout()\n",
    "plt.bar(keys, vals)\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Percent missing\")\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.title(\"Percent missing by column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NA values by health status\n",
    "tree_df[['health', 'status']].value_counts(dropna=False).plot(kind='bar')\n",
    "plt.title('Trees with NaN health')\n",
    "plt.ylabel(\"Number of trees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac51c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop dead trees that appear as NA\n",
    "tree_df_clean = tree_df[tree_df['health'].notna()]\n",
    "print(\"Dead or stumps:\", tree_df.shape[0] - tree_df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f616d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other NA values\n",
    "vals = (tree_df_clean.isna().sum() * 100 / len(tree_df_clean)).values\n",
    "keys = tree_df_clean.columns\n",
    "plt.tight_layout()\n",
    "plt.bar(keys, vals)\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Percent missing\")\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.title(\"Percent missing by column without dead trees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dropped columns:\n",
    "- borocode and boro_ct, equivalent to borough\n",
    "- x_sp and y_sp - use latitude and longitude instead\n",
    "- state - identical for all\n",
    "- nta_name, equivalent to nta\n",
    "- zip_city, directly correlated to postcode\n",
    "- address, estimated from latitude and longitude\n",
    "- spc_latin, equivalent to spc_common\n",
    "- created_at, not helpful (time data was added)\n",
    "- tree_id, not helpful (unique identifier)\n",
    "- block_id, not helpful (geo location)\n",
    "- user_type, not helpful (which volunteer analyzed it)\n",
    "- bin, not helpful (building identifier)\n",
    "- bbl, not helpful (block identifier)\n",
    "- council district, equivalent to cncldist\n",
    "- census tract, missing values and other location data\n",
    "- problems, one-hot encoded in other values\n",
    "- status, all are alive after dropping dead and stumps\n",
    "- stump_diam, only applies to stumps\n",
    "'''\n",
    "tree_df_dropped = tree_df_clean.drop(columns=[\n",
    "    'borocode', 'x_sp', 'y_sp', 'state', 'nta_name', 'zip_city', 'address',\n",
    "    'spc_latin', 'created_at', 'tree_id', 'block_id', 'user_type', 'bin',\n",
    "    'bbl', 'council district', 'boro_ct', 'census tract', 'problems', 'status',\n",
    "    'stump_diam'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa8fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remianing NA values\n",
    "vals = (tree_df_dropped.isna().sum() * 100 / len(tree_df_dropped)).values\n",
    "keys = tree_df_dropped.columns\n",
    "plt.tight_layout()\n",
    "plt.bar(keys, vals)\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Percent missing\")\n",
    "plt.xticks(rotation=90, fontsize=8)\n",
    "plt.title(\"Percent missing by column without dead trees and dropped columns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop those rows\n",
    "tree_df_processed = tree_df_dropped.dropna(how='any')\n",
    "print(\"Trees with missing values:\", tree_df_dropped.shape[0] - tree_df_processed.shape[0])\n",
    "miss_val = tree_df_processed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop invalid zipcode\n",
    "tree_df_processed = tree_df_processed[tree_df_processed['postcode'] != 83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12889d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each quantitaive feature\n",
    "vals = [\n",
    "    'tree_dbh', 'postcode', 'community board', 'cncldist', 'st_assem',\n",
    "    'st_senate', 'latitude', 'longitude'\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(len(vals) // 2, 2, figsize=(10, 12), constrained_layout=True)\n",
    "\n",
    "for i, val in enumerate(vals):\n",
    "    axs[i // 2, i % 2].hist(tree_df_processed[val], bins=50)\n",
    "    axs[i // 2, i % 2].set(xlabel=val, ylabel=\"Number of trees\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30febbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each categorical feature realtive to the health of the tree\n",
    "def cat_plot(feature, ax):\n",
    "    df = tree_df_processed[['health', feature]].value_counts().to_frame()\n",
    "    df = df.unstack(level=0)\n",
    "    df.columns = ['Fair', 'Good', 'Poor']\n",
    "    df.plot(kind='bar', ax=ax)\n",
    "\n",
    "vals = [\n",
    "    'guards', 'curb_loc', 'steward', 'sidewalk', 'root_stone', 'root_grate',\n",
    "    'root_other', 'trunk_wire', 'trnk_light', 'trnk_other', 'brch_light', 'brch_shoe',\n",
    "    'brch_other', 'borough'\n",
    "]\n",
    "    \n",
    "fig, axs = plt.subplots(len(vals) // 2, 2, figsize=(8, 18), constrained_layout=True)\n",
    "\n",
    "for i, val in enumerate(vals):\n",
    "    cat_plot(val, axs[i // 2, i % 2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d799114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for multicolinearity\n",
    "# Ordinal encoding\n",
    "tree_df_encoded = tree_df_processed.copy(deep=True)\n",
    "\n",
    "health = ['Poor', 'Fair', 'Good']\n",
    "enc = OrdinalEncoder(categories=[health])\n",
    "tree_df_encoded['health'] = enc.fit_transform(tree_df_processed[['health']])\n",
    "\n",
    "steward = ['None', '1or2', '3or4', '4orMore']\n",
    "enc = OrdinalEncoder(categories=[steward])\n",
    "tree_df_encoded['steward'] = enc.fit_transform(tree_df_encoded[['steward']])\n",
    "\n",
    "sidewalk = ['NoDamage', 'Damage']\n",
    "enc = OrdinalEncoder(categories=[sidewalk])\n",
    "tree_df_encoded['sidewalk'] = enc.fit_transform(tree_df_encoded[['sidewalk']])\n",
    "\n",
    "curbloc = ['OnCurb', 'OffsetFromCurb']\n",
    "enc = OrdinalEncoder(categories=[curbloc])\n",
    "tree_df_encoded['curb_loc'] = enc.fit_transform(tree_df_encoded[['curb_loc']])\n",
    "\n",
    "yes_no = ['No', 'Yes']\n",
    "enc = OrdinalEncoder(categories=[yes_no])\n",
    "\n",
    "yes_no_features = [\n",
    "    'brch_other', 'brch_shoe', 'brch_light', 'trnk_other', 'trnk_light',\n",
    "    'trunk_wire', 'root_other', 'root_grate', 'root_stone'\n",
    "]\n",
    "\n",
    "for feat in yes_no_features:\n",
    "    tree_df_encoded[feat] = enc.fit_transform(tree_df_encoded[[feat]])\n",
    "    \n",
    "# One-hot encode\n",
    "enc = OneHotEncoder()\n",
    "guards = enc.fit_transform(tree_df_encoded[['guards']])\n",
    "tree_df_encoded['guards_' + enc.categories_[0][:]] = guards.toarray()\n",
    "tree_df_encoded = tree_df_encoded.drop(columns=['guards'])\n",
    "\n",
    "guards = enc.fit_transform(tree_df_encoded[['borough']])\n",
    "tree_df_encoded['borough_' + enc.categories_[0][:]] = guards.toarray()\n",
    "tree_df_encoded = tree_df_encoded.drop(columns=['borough'])\n",
    "\n",
    "corr = tree_df_encoded.corr()\n",
    "\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check species feature for usability\n",
    "tree_df_encoded['spc_common'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327c882",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data with relative path\n",
    "tree_df = pd.read_csv('2015_Street_Tree_Census_-_Tree_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "tree_df = tree_df.drop(columns=[\n",
    "    'borocode', 'x_sp', 'y_sp', 'state', 'nta_name', 'zip_city', 'address',\n",
    "    'spc_latin', 'created_at', 'tree_id', 'block_id', 'user_type', 'bin',\n",
    "    'bbl', 'council district', 'boro_ct', 'census tract', 'problems', 'status',\n",
    "    'stump_diam', 'postcode', 'community board', 'cncldist', 'st_assem', 'st_senate', \n",
    "    'nta', 'spc_common'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40691cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NA and dead tree values\n",
    "tree_df = tree_df[tree_df['health'].notna()]\n",
    "tree_df = tree_df.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale diameters \n",
    "scaler = StandardScaler()\n",
    "tree_df['tree_dbh'] = scaler.fit_transform(tree_df[['tree_dbh']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2275be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encode\n",
    "health = ['Poor', 'Fair', 'Good']\n",
    "enc = OrdinalEncoder(categories=[health])\n",
    "tree_df['health'] = enc.fit_transform(tree_df[['health']])\n",
    "\n",
    "steward = ['None', '1or2', '3or4', '4orMore']\n",
    "enc = OrdinalEncoder(categories=[steward])\n",
    "tree_df['steward'] = enc.fit_transform(tree_df[['steward']])\n",
    "\n",
    "sidewalk = ['NoDamage', 'Damage']\n",
    "enc = OrdinalEncoder(categories=[sidewalk])\n",
    "tree_df['sidewalk'] = enc.fit_transform(tree_df[['sidewalk']])\n",
    "\n",
    "curbloc = ['OnCurb', 'OffsetFromCurb']\n",
    "enc = OrdinalEncoder(categories=[curbloc])\n",
    "tree_df['curb_loc'] = enc.fit_transform(tree_df[['curb_loc']])\n",
    "\n",
    "yes_no = ['No', 'Yes']\n",
    "enc = OrdinalEncoder(categories=[yes_no])\n",
    "\n",
    "yes_no_features = [\n",
    "    'brch_other', 'brch_shoe', 'brch_light', 'trnk_other', 'trnk_light',\n",
    "    'trunk_wire', 'root_other', 'root_grate', 'root_stone'\n",
    "]\n",
    "\n",
    "for feat in yes_no_features:\n",
    "    tree_df[feat] = enc.fit_transform(tree_df[[feat]])\n",
    "    \n",
    "# One Hot Encode\n",
    "enc = OneHotEncoder()\n",
    "guards = enc.fit_transform(tree_df[['guards']])\n",
    "tree_df['guards_' + enc.categories_[0][:]] = guards.toarray()\n",
    "tree_df = tree_df.drop(columns=['guards'])\n",
    "\n",
    "guards = enc.fit_transform(tree_df[['borough']])\n",
    "tree_df['borough_' + enc.categories_[0][:]] = guards.toarray()\n",
    "tree_df = tree_df.drop(columns=['borough'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fba5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to x and y sets\n",
    "y = np.asarray(tree_df['health'])\n",
    "x = tree_df.drop(columns=['health'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a602a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random sample (stratified) for dev, test, train and val\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify = y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_dev, y_dev, test_size=0.2, random_state=42, stratify = y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df78bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# over-sampling of training data\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_ros, y_ros = ros.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc51372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under sampling of training data\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_rus, y_rus = rus.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote sampling of training data\n",
    "smote = SMOTE(random_state=42)\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14add3d1",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10f54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "273fcf2c",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e07102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model, x_train, y_train, x_test, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_train_predict = model.predict(x_train)\n",
    "    y_test_predict = model.predict(x_test)    \n",
    "    # accuracy scores\n",
    "    acc_train = accuracy_score(y_train,y_train_predict)\n",
    "    acc_test = accuracy_score(y_test,y_test_predict)\n",
    "    # print trainind and testing accuracy\n",
    "    print(\"Accuracy of Training data: \", acc_train)\n",
    "    print(\"Accuracy of Testing data: \", acc_test)\n",
    "    # print train and test f1 score\n",
    "    f1_train = f1_score(y_train, y_train_predict , average='weighted')\n",
    "    f1_test = f1_score(y_test, y_test_predict , average='weighted')\n",
    "    print(\"weigted f1 of Training data: \", f1_train)\n",
    "    print(\"weigted f1 of Testing data: \", f1_test)\n",
    "\n",
    "    # confusion_matrix\n",
    "    cf = confusion_matrix(y_test, model.predict(x_test))\n",
    "    sns.heatmap(cf, annot = True, fmt = 'g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452073fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_select(x_train,y_train,score):\n",
    "    param_grid = {\n",
    "        'max_depth': [4,8,10,12,15,20],\n",
    "        'min_samples_split': np.linspace(0.1, 0.3, 3, endpoint=True),\n",
    "        \"max_features\":[\"log2\",\"sqrt\",None],\n",
    "        \"max_leaf_nodes\": [None,10,20,30],\n",
    "        'ccp_alpha' : np.logspace(-6, -4, 3)\n",
    "    }\n",
    "    # Create a based model\n",
    "    dt = DecisionTreeClassifier(random_state=100)\n",
    "\n",
    "    # Instantiate the random search model\n",
    "    randomized_search = RandomizedSearchCV(estimator = dt, param_distributions = param_grid, scoring=score,cv=5)\n",
    "\n",
    "    randomized_model=randomized_search.fit(x_train,y_train)\n",
    "    return randomized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff88a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random stratified sampling\n",
    "# accuracy as scoring metric\n",
    "random_best = dt_select(x_train,y_train,'accuracy')\n",
    "print(\"optimal hyperparameters\",random_best.best_params_)\n",
    "print(\"optimal accuracy score\", random_best.best_score_)\n",
    "print(\"test accuracy score\",random_best.score(x_test,y_test))\n",
    "#f1 as metric for hyperparamter tuning\n",
    "model_result(random_best, x_train, y_train, x_test, y_test)\n",
    "#f1 as scoring metric\n",
    "random_best_f1 = dt_select(x_train,y_train,'f1_weighted')\n",
    "print(\"optimal hyperparameters\",random_best_f1.best_params_)\n",
    "print(\"optimal f1 score\", random_best_f1.best_score_)\n",
    "print(\"test f1 score\",random_best_f1.score(x_test,y_test))\n",
    "#f1 as metric for hyperparamter tuning\n",
    "model_result(random_best_f1, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random over sampling\n",
    "# accuracy as scoring metric\n",
    "over_best = dt_select(x_ros, y_ros,'accuracy')\n",
    "print(\"optimal hyperparameters\",over_best.best_params_)\n",
    "print(\"optimal accuracy score\", over_best.best_score_)\n",
    "print(\"test accuracy\",over_best.score(x_test,y_test))\n",
    "#accuracy as metric for hyperparamter tuning\n",
    "model_result(over_best, x_ros, y_ros, x_test, y_test)\n",
    "# f1 as score metric\n",
    "over_best_f1 = dt_select(x_ros, y_ros,'f1_weighted')\n",
    "print(\"optimal hyperparameters\",over_best_f1.best_params_)\n",
    "print(\"optimal accuracy score\", over_best_f1.best_score_)\n",
    "print(\"test accuracy\",over_best_f1.score(x_test,y_test))\n",
    "#f1 as metric for hyperparamter tuning\n",
    "model_result(over_best_f1, x_ros, y_ros, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946bf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random under sampling\n",
    "# accuracy as scoring metric\n",
    "under_best = dt_select(x_rus, y_rus,'accuracy')\n",
    "print(\"optimal hyperparameters\",under_best.best_params_)\n",
    "print(\"optimal accuracy score\", under_best.best_score_)\n",
    "print(\"test accuracy\",under_best.score(x_test,y_test))\n",
    "# accuracy as metric for hyperparamter tuning\n",
    "model_result(under_best, x_rus, y_rus, x_test, y_test)\n",
    "# f1 as score metric\n",
    "under_best_f1 = dt_select(x_rus, y_rus,'f1_weighted')\n",
    "print(\"optimal hyperparameters\",under_best_f1.best_params_)\n",
    "print(\"optimal f1 score\", under_best_f1.best_score_)\n",
    "print(\"test f1 score\",under_best_f1.score(x_test,y_test))\n",
    "# f1 as metric for hyperparamter pruning\n",
    "model_result(under_best_f1, x_rus, y_rus, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632b22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smote sampling\n",
    "# accuracy as metric\n",
    "smote_best = dt_select(x_smote, y_smote,\"accuracy\")\n",
    "print(\"optimal hyperparameters\",smote_best.best_params_)\n",
    "print(\"optimal accuracy score\", smote_best.best_score_)\n",
    "print(\"test accuracy\",smote_best.score(x_test,y_test))\n",
    "#accuracy as metric for hyperparamter pruning\n",
    "model_result(smote_best, x_smote, y_smote, x_test, y_test)\n",
    "# f1 sccore as metric\n",
    "smote_best_f1 = dt_select(x_smote, y_smote,'f1_weighted')\n",
    "print(\"optimal hyperparameters\",smote_best_f1.best_params_)\n",
    "print(\"optimal f1 score\", smote_best_f1.best_score_)\n",
    "print(\"test f1 score\",smote_best_f1.score(x_test,y_test))\n",
    "# f1 as metric for hyperparamter pruning\n",
    "model_result(smote_best_f1, x_smote, y_smote, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b917a1a",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e767d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_result(svm, X_train, y_train, X_test, y_test, report = False, cf = False):\n",
    "    # fit the model\n",
    "    svm.fit(X_train, y_train)\n",
    "    # predict y\n",
    "    y_train_predict = svm.predict(X_train)\n",
    "    y_test_predict = svm.predict(X_test)    \n",
    "    # accuracy scores\n",
    "    acc_predit_train = accuracy_score(y_train_predict,y_train)\n",
    "    acc_predit_test = accuracy_score(y_test_predict,y_test)\n",
    "    # print trainind and testing accuracy\n",
    "    print(\"Accuracy of Training data: \", acc_predit_train)\n",
    "    print(\"Accuracy of Testing data: \", acc_predit_test)\n",
    "    # show result report\n",
    "    if (report):\n",
    "        print(classification_report(y_test, y_test_predict))\n",
    "    # show confusion_matrix\n",
    "    if (cf):\n",
    "        cf = confusion_matrix(y_test, primal_svm.predict(X_test))\n",
    "        sns.heatmap(cf, annot = True, fmt = 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10524759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SVM models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# basic primal and dual svm before hyperparameter tunin\n",
    "svm = SVC(random_state=100)\n",
    "# random search for hyperparameter tuning\n",
    "param_list = {'C': stats.uniform(0, 5), \n",
    "              'kernel': ['linear', 'poly', 'sigmoid', 'rbf'], \n",
    "              'gamma': [.01, 1, 10, 100]} # random list for hyperparameter tuning\n",
    "rand_search = RandomizedSearchCV(svm, \n",
    "                                 param_distributions = param_list, \n",
    "                                 cv = 5, \n",
    "                                 random_state = 2022) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe358d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning random Sampling\n",
    "rand_search.fit(x_train, y_train) \n",
    "svm_best = rand_search.best_estimator_\n",
    "print(svm_best)\n",
    "SVM_result(svm_best, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning under sampling\n",
    "rand_search.fit(x_rus, y_rus) \n",
    "svm_best = rand_search.best_estimator_\n",
    "print(svm_best)\n",
    "SVM_result(svm_best, x_rus, y_rus, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search.fit(x_ros, y_ros)\n",
    "svm_best = rand_search.best_estimator_\n",
    "print(svm_best)\n",
    "SVM_result(svm_best, x_ros, y_ros, x_test, y_test, cf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299edaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_search.fit(x_smote, y_smote)\n",
    "svm_best = rand_search.best_estimator_\n",
    "print(svm_best)\n",
    "SVM_result(svm_best, x_smote, y_smote, x_test, y_test, cf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ca093",
   "metadata": {},
   "source": [
    "## Tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f284b3d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
